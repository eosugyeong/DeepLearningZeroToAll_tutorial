{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.545824 [[ 0.27378842  0.22246814  0.09797081]\n",
      " [ 0.35881877  0.02296349  0.08982889]\n",
      " [-1.5701909   0.92923474 -1.18008   ]]\n",
      "1 6.416433 [[ 0.29833704  0.16069007  0.13520025]\n",
      " [ 0.52036655 -0.23800981  0.1892544 ]\n",
      " [-1.4081924   0.6800414  -1.0928851 ]]\n",
      "2 4.3133945 [[ 0.32140112  0.10108462  0.1717416 ]\n",
      " [ 0.6784338  -0.49389404  0.28707144]\n",
      " [-1.2481923   0.433779   -1.0066229 ]]\n",
      "3 2.3544178 [[ 0.33644518  0.0518955   0.20588665]\n",
      " [ 0.8041041  -0.70925105  0.37675816]\n",
      " [-1.1110885   0.21614704 -0.9260947 ]]\n",
      "4 1.4295547 [[ 0.31779864  0.04255784  0.23387086]\n",
      " [ 0.74524564 -0.7113881   0.43775365]\n",
      " [-1.1365464   0.18737788 -0.8718677 ]]\n",
      "5 1.3369031 [[ 0.3085873   0.03332054  0.25231948]\n",
      " [ 0.7381953  -0.71424943  0.44766533]\n",
      " [-1.1162302   0.15855277 -0.86335874]]\n",
      "6 1.3177264 [[ 0.2981797   0.02796814  0.2680795 ]\n",
      " [ 0.7244256  -0.6965917   0.4437773 ]\n",
      " [-1.1032894   0.15017952 -0.8679263 ]]\n",
      "7 1.3026255 [[ 0.28864327  0.02175138  0.28383267]\n",
      " [ 0.7148167  -0.6835948   0.44038936]\n",
      " [-1.086768    0.13744661 -0.87171465]]\n",
      "8 1.2879584 [[ 0.27891523  0.01577977  0.29953232]\n",
      " [ 0.7035711  -0.6693978   0.43743792]\n",
      " [-1.0721254   0.12612127 -0.8750318 ]]\n",
      "9 1.2735988 [[ 0.26945984  0.0097185   0.31504896]\n",
      " [ 0.6932729  -0.6557963   0.4341347 ]\n",
      " [-1.0569662   0.11444651 -0.8785163 ]]\n",
      "10 1.259532 [[ 0.26006696  0.00369932  0.33046103]\n",
      " [ 0.6827701  -0.64210063  0.43094188]\n",
      " [-1.0423503   0.10310006 -0.88178575]]\n",
      "11 1.2457523 [[ 0.25082675 -0.00233128  0.34573185]\n",
      " [ 0.6725792  -0.6286107   0.42764273]\n",
      " [-1.0278      0.09179135 -0.8850274 ]]\n",
      "12 1.2322555 [[ 0.24169414 -0.00834596  0.36087915]\n",
      " [ 0.66246545 -0.61519337  0.42433926]\n",
      " [-1.0135297   0.08065345 -0.8881598 ]]\n",
      "13 1.2190368 [[ 0.23268561 -0.01435434  0.37589607]\n",
      " [ 0.652536   -0.6019138   0.4209892 ]\n",
      " [-0.9994385   0.06962573 -0.89122325]]\n",
      "14 1.2060925 [[ 0.22378926 -0.02034879  0.39078686]\n",
      " [ 0.6427405  -0.5887445   0.41761535]\n",
      " [-0.98557115  0.05873828 -0.8942032 ]]\n",
      "15 1.1934186 [[ 0.21500638 -0.0263299   0.40555084]\n",
      " [ 0.6331022  -0.5757019   0.41421106]\n",
      " [-0.9719043   0.04797831 -0.89711004]]\n",
      "16 1.181011 [[ 0.20633206 -0.03229439  0.42018965]\n",
      " [ 0.6236096  -0.5627814   0.41078314]\n",
      " [-0.9584468   0.0373536  -0.8999428 ]]\n",
      "17 1.1688656 [[ 0.19776434 -0.03824088  0.43470386]\n",
      " [ 0.6142679  -0.54998875  0.40733218]\n",
      " [-0.9451922   0.026862   -0.9027058 ]]\n",
      "18 1.1569791 [[ 0.18929984 -0.04416712  0.4490946 ]\n",
      " [ 0.6050742  -0.53732455  0.4038616 ]\n",
      " [-0.9321415   0.0165064  -0.9054009 ]]\n",
      "19 1.1453475 [[ 0.1809359  -0.05007137  0.46336275]\n",
      " [ 0.5960298  -0.52479213  0.40037358]\n",
      " [-0.919292    0.00628712 -0.9080311 ]]\n",
      "20 1.133967 [[ 0.17266954 -0.05595164  0.4775094 ]\n",
      " [ 0.5871338  -0.5123933   0.39687076]\n",
      " [-0.9066429  -0.00379405 -0.91059893]]\n",
      "21 1.122834 [[ 0.16449796 -0.06180618  0.49153554]\n",
      " [ 0.5783865  -0.5001308   0.39335558]\n",
      " [-0.8941927  -0.01373612 -0.9131071 ]]\n",
      "22 1.1119453 [[ 0.1564183  -0.06763315  0.5054422 ]\n",
      " [ 0.56978756 -0.48800683  0.38983053]\n",
      " [-0.88194007 -0.02353762 -0.9155582 ]]\n",
      "23 1.1012968 [[ 0.14842777 -0.07343084  0.5192304 ]\n",
      " [ 0.56133705 -0.47602385  0.38629803]\n",
      " [-0.8698837  -0.03319731 -0.91795486]]\n",
      "24 1.0908852 [[ 0.14052364 -0.07919754  0.5329013 ]\n",
      " [ 0.55303496 -0.46418422  0.38276047]\n",
      " [-0.85802233 -0.04271392 -0.92029965]]\n",
      "25 1.0807071 [[ 0.13270317 -0.08493159  0.5464558 ]\n",
      " [ 0.54488117 -0.45249015  0.3792202 ]\n",
      " [-0.84635466 -0.05208604 -0.92259514]]\n",
      "26 1.0707586 [[ 0.1249637  -0.0906314   0.5598951 ]\n",
      " [ 0.53687567 -0.440944    0.3756795 ]\n",
      " [-0.83487946 -0.0613125  -0.9248439 ]]\n",
      "27 1.0610368 [[ 0.11730261 -0.09629543  0.5732202 ]\n",
      " [ 0.52901846 -0.42954794  0.37214065]\n",
      " [-0.82359546 -0.07039203 -0.9270483 ]]\n",
      "28 1.051538 [[ 0.10971731 -0.10192217  0.5864322 ]\n",
      " [ 0.52130944 -0.4183041   0.36860585]\n",
      " [-0.8125015  -0.07932344 -0.9292109 ]]\n",
      "29 1.0422587 [[ 0.10220529 -0.10751019  0.59953225]\n",
      " [ 0.5137486  -0.40721455  0.3650772 ]\n",
      " [-0.8015963  -0.08810559 -0.931334  ]]\n",
      "30 1.0331955 [[ 0.0947641  -0.11305811  0.61252135]\n",
      " [ 0.50633585 -0.3962814   0.36155677]\n",
      " [-0.7908785  -0.09673747 -0.93341994]]\n",
      "31 1.0243448 [[ 0.08739129 -0.11856455  0.6254006 ]\n",
      " [ 0.499071   -0.3855063   0.35804653]\n",
      " [-0.780347   -0.10521791 -0.935471  ]]\n",
      "32 1.0157032 [[ 0.08008453 -0.1240283   0.63817114]\n",
      " [ 0.491954   -0.37489125  0.35454845]\n",
      " [-0.77000046 -0.11354618 -0.9374893 ]]\n",
      "33 1.007267 [[ 0.07284155 -0.12944813  0.65083396]\n",
      " [ 0.48498467 -0.36443782  0.35106435]\n",
      " [-0.7598375  -0.12172136 -0.939477  ]]\n",
      "34 0.999033 [[ 0.06566008 -0.13482286  0.66339016]\n",
      " [ 0.47816265 -0.3541475   0.34759605]\n",
      " [-0.74985695 -0.12974268 -0.94143623]]\n",
      "35 0.9909972 [[ 0.05853801 -0.14015143  0.6758408 ]\n",
      " [ 0.47148782 -0.34402183  0.3441452 ]\n",
      " [-0.7400573  -0.13760965 -0.9433689 ]]\n",
      "36 0.9831562 [[ 0.0514732  -0.14543279  0.68818694]\n",
      " [ 0.46495968 -0.33406195  0.34071344]\n",
      " [-0.7304372  -0.14532164 -0.945277  ]]\n",
      "37 0.97550654 [[ 0.04446365 -0.15066597  0.7004297 ]\n",
      " [ 0.45857793 -0.32426906  0.3373023 ]\n",
      " [-0.72099525 -0.15287837 -0.9471622 ]]\n",
      "38 0.9680441 [[ 0.0375074  -0.15585007  0.71257   ]\n",
      " [ 0.452342   -0.31464404  0.33391318]\n",
      " [-0.7117299  -0.16027953 -0.94902647]]\n",
      "39 0.9607655 [[ 0.03060254 -0.16098425  0.7246091 ]\n",
      " [ 0.4462514  -0.30518776  0.3305475 ]\n",
      " [-0.7026395  -0.16752505 -0.9508713 ]]\n",
      "40 0.9536666 [[ 0.02374727 -0.16606773  0.7365478 ]\n",
      " [ 0.44030544 -0.29590082  0.32720652]\n",
      " [-0.6937226  -0.1746149  -0.95269835]]\n",
      "41 0.9467441 [[ 0.01693982 -0.1710998   0.74838734]\n",
      " [ 0.4345034  -0.28678367  0.3238914 ]\n",
      " [-0.6849774  -0.18154928 -0.95450914]]\n",
      "42 0.9399938 [[ 0.01017851 -0.17607978  0.7601286 ]\n",
      " [ 0.42884448 -0.2778366   0.32060325]\n",
      " [-0.6764023  -0.18832847 -0.9563051 ]]\n",
      "43 0.93341196 [[ 0.00346173 -0.1810071   0.77177274]\n",
      " [ 0.4233278  -0.26905978  0.31734312]\n",
      " [-0.66799533 -0.19495296 -0.9580875 ]]\n",
      "44 0.9269947 [[-0.00321208 -0.18588123  0.78332067]\n",
      " [ 0.41795236 -0.26045305  0.31411183]\n",
      " [-0.65975475 -0.20142329 -0.95985776]]\n",
      "45 0.9207382 [[-0.00984439 -0.1907017   0.79477346]\n",
      " [ 0.4127171  -0.25201628  0.3109103 ]\n",
      " [-0.65167856 -0.20774025 -0.961617  ]]\n",
      "46 0.9146386 [[-0.01643662 -0.19546808  0.8061321 ]\n",
      " [ 0.40762076 -0.24374898  0.30773935]\n",
      " [-0.6437649  -0.21390465 -0.96336627]]\n",
      "47 0.90869176 [[-0.02299009 -0.20018005  0.81739753]\n",
      " [ 0.40266225 -0.23565066  0.30459952]\n",
      " [-0.63601154 -0.2199176  -0.96510667]]\n",
      "48 0.9028941 [[-0.02950613 -0.2048373   0.82857084]\n",
      " [ 0.39784005 -0.22772044  0.3014915 ]\n",
      " [-0.6284165  -0.22578013 -0.9668392 ]]\n",
      "49 0.89724153 [[-0.03598592 -0.20943962  0.83965296]\n",
      " [ 0.39315283 -0.21995752  0.2984158 ]\n",
      " [-0.6209775  -0.23149365 -0.9685647 ]]\n",
      "50 0.8917302 [[-0.04243064 -0.2139868   0.8506448 ]\n",
      " [ 0.38859904 -0.21236078  0.29537287]\n",
      " [-0.61369234 -0.23705952 -0.970284  ]]\n",
      "51 0.8863563 [[-0.04884138 -0.21847872  0.86154747]\n",
      " [ 0.38417703 -0.204929    0.29236308]\n",
      " [-0.60655874 -0.24247926 -0.97199786]]\n",
      "52 0.88111585 [[-0.05521918 -0.22291532  0.8723619 ]\n",
      " [ 0.37988517 -0.1976608   0.28938675]\n",
      " [-0.5995743  -0.24775457 -0.97370696]]\n",
      "53 0.87600535 [[-0.06156501 -0.22729656  0.88308895]\n",
      " [ 0.37572172 -0.19055466  0.28644407]\n",
      " [-0.59273666 -0.2528872  -0.975412  ]]\n",
      "54 0.8710207 [[-0.06787982 -0.23162247  0.8937297 ]\n",
      " [ 0.37168476 -0.18360892  0.28353527]\n",
      " [-0.58604336 -0.25787896 -0.97711354]]\n",
      "55 0.86615837 [[-0.07416447 -0.23589312  0.90428495]\n",
      " [ 0.36777246 -0.17682181  0.28066048]\n",
      " [-0.57949185 -0.2627319  -0.9788121 ]]\n",
      "56 0.8614147 [[-0.08041979 -0.24010861  0.91475576]\n",
      " [ 0.36398274 -0.1701914   0.27781978]\n",
      " [-0.5730797  -0.26744804 -0.98050815]]\n",
      "57 0.8567859 [[-0.08664654 -0.2442691   0.925143  ]\n",
      " [ 0.36031368 -0.16371576  0.27501318]\n",
      " [-0.56680423 -0.27202958 -0.9822021 ]]\n",
      "58 0.8522687 [[-0.09284548 -0.24837479  0.93544763]\n",
      " [ 0.35676312 -0.15739267  0.27224064]\n",
      " [-0.560663   -0.27647862 -0.98389435]]\n",
      "59 0.8478594 [[-0.09901726 -0.2524259   0.94567055]\n",
      " [ 0.353329   -0.15122     0.26950207]\n",
      " [-0.5546532  -0.28079757 -0.9855852 ]]\n",
      "60 0.8435546 [[-0.10516255 -0.2564227   0.95581263]\n",
      " [ 0.3500091  -0.14519541  0.2667974 ]\n",
      " [-0.54877234 -0.2849887  -0.98727494]]\n",
      "61 0.8393512 [[-0.11128195 -0.2603655   0.9658748 ]\n",
      " [ 0.34680122 -0.13931653  0.26412642]\n",
      " [-0.5430177  -0.28905442 -0.98896384]]\n",
      "62 0.8352457 [[-0.11737601 -0.2642546   0.975858  ]\n",
      " [ 0.3437031  -0.13358095  0.26148897]\n",
      " [-0.5373866  -0.2929972  -0.99065214]]\n",
      "63 0.8312348 [[-0.12344527 -0.26809037  0.985763  ]\n",
      " [ 0.34071243 -0.12798615  0.25888482]\n",
      " [-0.53187644 -0.2968195  -0.99234   ]]\n",
      "64 0.8273156 [[-0.12949021 -0.2718732   0.9955908 ]\n",
      " [ 0.33782697 -0.12252962  0.25631374]\n",
      " [-0.52648443 -0.30052394 -0.9940276 ]]\n",
      "65 0.82348526 [[-0.13551132 -0.2756035   1.0053422 ]\n",
      " [ 0.3350443  -0.11720862  0.25377542]\n",
      " [-0.52120805 -0.30411285 -0.9957151 ]]\n",
      "66 0.8197404 [[-0.141509   -0.2792817   1.0150181 ]\n",
      " [ 0.33236226 -0.11202072  0.25126955]\n",
      " [-0.51604444 -0.30758902 -0.9974025 ]]\n",
      "67 0.8160785 [[-0.14748368 -0.28290826  1.0246193 ]\n",
      " [ 0.3297783  -0.10696312  0.24879587]\n",
      " [-0.51099116 -0.3109549  -0.9990899 ]]\n",
      "68 0.81249666 [[-0.15343572 -0.28648365  1.0341468 ]\n",
      " [ 0.32729027 -0.10203318  0.24635397]\n",
      " [-0.5060454  -0.31421313 -1.0007775 ]]\n",
      "69 0.8089922 [[-0.15936549 -0.29000834  1.0436013 ]\n",
      " [ 0.32489568 -0.09722815  0.24394353]\n",
      " [-0.5012046  -0.31736627 -1.0024651 ]]\n",
      "70 0.8055626 [[-0.16527331 -0.29348284  1.0529836 ]\n",
      " [ 0.32259226 -0.09254535  0.24156414]\n",
      " [-0.49646616 -0.3204169  -1.0041529 ]]\n",
      "71 0.80220526 [[-0.17115949 -0.2969077   1.0622947 ]\n",
      " [ 0.32037765 -0.08798205  0.23921545]\n",
      " [-0.4918275  -0.3233676  -1.0058409 ]]\n",
      "72 0.7989178 [[-0.17702432 -0.30028343  1.0715352 ]\n",
      " [ 0.31824955 -0.08353559  0.23689707]\n",
      " [-0.48728603 -0.326221   -1.007529  ]]\n",
      "73 0.7956979 [[-0.18286806 -0.30361056  1.0807061 ]\n",
      " [ 0.31620565 -0.07920317  0.23460858]\n",
      " [-0.48283926 -0.32897952 -1.0092173 ]]\n",
      "74 0.7925433 [[-0.18869099 -0.30688965  1.0898081 ]\n",
      " [ 0.31424367 -0.07498218  0.23234956]\n",
      " [-0.4784847  -0.3316458  -1.0109055 ]]\n",
      "75 0.7894517 [[-0.19449332 -0.31012127  1.098842  ]\n",
      " [ 0.31236136 -0.07086989  0.2301196 ]\n",
      " [-0.47421983 -0.3342223  -1.0125939 ]]\n",
      "76 0.7864213 [[-0.2002753  -0.31330597  1.1078087 ]\n",
      " [ 0.3105564  -0.06686363  0.2279183 ]\n",
      " [-0.47004235 -0.33671147 -1.0142822 ]]\n",
      "77 0.78344977 [[-0.20603712 -0.31644434  1.1167089 ]\n",
      " [ 0.30882668 -0.06296084  0.22574522]\n",
      " [-0.46594977 -0.33911583 -1.0159705 ]]\n",
      "78 0.78053534 [[-0.21177898 -0.31953695  1.1255434 ]\n",
      " [ 0.30717    -0.05915885  0.22359993]\n",
      " [-0.46193978 -0.3414377  -1.0176586 ]]\n",
      "79 0.77767605 [[-0.21750107 -0.3225844   1.1343129 ]\n",
      " [ 0.30558416 -0.05545507  0.221482  ]\n",
      " [-0.45801008 -0.3436795  -1.0193465 ]]\n",
      "80 0.7748701 [[-0.22320358 -0.3255872   1.1430182 ]\n",
      " [ 0.30406708 -0.05184699  0.21939102]\n",
      " [-0.4541584  -0.34584352 -1.0210341 ]]\n",
      "81 0.7721159 [[-0.22888666 -0.32854602  1.1516601 ]\n",
      " [ 0.30261666 -0.04833213  0.21732657]\n",
      " [-0.45038253 -0.34793216 -1.0227214 ]]\n",
      "82 0.7694117 [[-0.23455049 -0.3314614   1.1602392 ]\n",
      " [ 0.30123085 -0.04490799  0.21528824]\n",
      " [-0.4466803  -0.3499476  -1.0244082 ]]\n",
      "83 0.7667558 [[-0.2401952  -0.33433393  1.1687565 ]\n",
      " [ 0.29990768 -0.04157215  0.21327558]\n",
      " [-0.44304955 -0.3518921  -1.0260944 ]]\n",
      "84 0.7641468 [[-0.24582094 -0.33716416  1.1772125 ]\n",
      " [ 0.2986451  -0.03832218  0.21128818]\n",
      " [-0.43948826 -0.35376775 -1.02778   ]]\n",
      "85 0.7615832 [[-0.25142783 -0.33995274  1.1856079 ]\n",
      " [ 0.29744133 -0.03515584  0.20932563]\n",
      " [-0.4359943  -0.35557678 -1.029465  ]]\n",
      "86 0.7590636 [[-0.25701606 -0.3427002   1.1939436 ]\n",
      " [ 0.2962943  -0.03207073  0.20738754]\n",
      " [-0.43256578 -0.35732117 -1.031149  ]]\n",
      "87 0.75658673 [[-0.2625857  -0.34540716  1.2022202 ]\n",
      " [ 0.29520229 -0.02906466  0.2054735 ]\n",
      " [-0.42920068 -0.35900304 -1.0328323 ]]\n",
      "88 0.75415105 [[-0.26813692 -0.34807414  1.2104384 ]\n",
      " [ 0.2941634  -0.02613542  0.20358312]\n",
      " [-0.42589712 -0.36062434 -1.0345145 ]]\n",
      "89 0.7517555 [[-0.2736698  -0.35070175  1.2185988 ]\n",
      " [ 0.29317594 -0.02328084  0.201716  ]\n",
      " [-0.4226533  -0.36218703 -1.0361958 ]]\n",
      "90 0.7493988 [[-0.2791845  -0.35329053  1.2267023 ]\n",
      " [ 0.29223818 -0.02049883  0.19987176]\n",
      " [-0.4194673  -0.363693   -1.0378758 ]]\n",
      "91 0.74707997 [[-0.28468108 -0.35584107  1.2347494 ]\n",
      " [ 0.29134837 -0.01778728  0.19805004]\n",
      " [-0.4163375  -0.36514404 -1.0395545 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 0.7447977 [[-0.29015967 -0.3583539   1.2427409 ]\n",
      " [ 0.29050496 -0.01514423  0.19625038]\n",
      " [-0.41326204 -0.36654204 -1.041232  ]]\n",
      "93 0.7425511 [[-0.29562038 -0.3608296   1.2506772 ]\n",
      " [ 0.2897063  -0.0125677   0.19447252]\n",
      " [-0.41023934 -0.36788872 -1.042908  ]]\n",
      "94 0.74033886 [[-0.30106333 -0.36326867  1.2585592 ]\n",
      " [ 0.2889508  -0.01005574  0.19271605]\n",
      " [-0.40726775 -0.36918575 -1.0445825 ]]\n",
      "95 0.73816025 [[-0.3064886  -0.3656717   1.2663875 ]\n",
      " [ 0.288237   -0.00760653  0.19098061]\n",
      " [-0.4043457  -0.37043485 -1.0462555 ]]\n",
      "96 0.7360143 [[-0.31189632 -0.36803916  1.2741627 ]\n",
      " [ 0.2875634  -0.0052182   0.18926588]\n",
      " [-0.40147164 -0.37163758 -1.0479268 ]]\n",
      "97 0.73390007 [[-0.31728655 -0.37037164  1.2818854 ]\n",
      " [ 0.2869286  -0.00288903  0.1875715 ]\n",
      " [-0.3986441  -0.37279558 -1.0495963 ]]\n",
      "98 0.7318169 [[-3.2265940e-01 -3.7266964e-01  1.2895563e+00]\n",
      " [ 2.8633118e-01 -6.1722659e-04  1.8589711e-01]\n",
      " [-3.9586157e-01 -3.7391031e-01 -1.0512640e+00]]\n",
      "99 0.72976345 [[-0.328015   -0.37493366  1.2971759 ]\n",
      " [ 0.2857698   0.00159885  0.18424241]\n",
      " [-0.39312267 -0.3749833  -1.05293   ]]\n",
      "100 0.72773945 [[-0.33335343 -0.3771642   1.3047448 ]\n",
      " [ 0.2852431   0.00376085  0.1826071 ]\n",
      " [-0.39042607 -0.376016   -1.0545939 ]]\n",
      "101 0.7257439 [[-0.33867478 -0.3793618   1.3122638 ]\n",
      " [ 0.28474987  0.00587036  0.18099082]\n",
      " [-0.38777035 -0.37700975 -1.0562558 ]]\n",
      "102 0.72377604 [[-0.34397918 -0.38152695  1.3197334 ]\n",
      " [ 0.2842888   0.00792897  0.17939328]\n",
      " [-0.3851543  -0.37796593 -1.0579157 ]]\n",
      "103 0.72183514 [[-0.34926668 -0.3836601   1.327154  ]\n",
      " [ 0.28385878  0.00993812  0.17781417]\n",
      " [-0.38257658 -0.3788859  -1.0595734 ]]\n",
      "104 0.7199206 [[-0.3545374  -0.38576177  1.3345264 ]\n",
      " [ 0.28345853  0.01189931  0.1762532 ]\n",
      " [-0.38003606 -0.37977087 -1.061229  ]]\n",
      "105 0.7180317 [[-0.35979143 -0.38783237  1.3418511 ]\n",
      " [ 0.283087    0.01381396  0.1747101 ]\n",
      " [-0.3775315  -0.38062212 -1.0628823 ]]\n",
      "106 0.716168 [[-0.36502886 -0.38987243  1.3491286 ]\n",
      " [ 0.28274304  0.01568344  0.17318456]\n",
      " [-0.3750618  -0.38144085 -1.0645332 ]]\n",
      "107 0.7143285 [[-0.37024978 -0.3918824   1.3563595 ]\n",
      " [ 0.28242567  0.01750909  0.1716763 ]\n",
      " [-0.3726258  -0.38222823 -1.0661819 ]]\n",
      "108 0.71251297 [[-0.3754543  -0.3938627   1.3635443 ]\n",
      " [ 0.28213376  0.01929228  0.17018504]\n",
      " [-0.3702225  -0.3829853  -1.0678282 ]]\n",
      "109 0.7107206 [[-0.38064253 -0.3958138   1.3706837 ]\n",
      " [ 0.28186643  0.0210341   0.16871056]\n",
      " [-0.36785075 -0.3837133  -1.069472  ]]\n",
      "110 0.7089509 [[-0.38581455 -0.3977361   1.377778  ]\n",
      " [ 0.2816226   0.02273598  0.16725254]\n",
      " [-0.36550966 -0.38441312 -1.0711132 ]]\n",
      "111 0.7072035 [[-0.39097044 -0.3996301   1.384828  ]\n",
      " [ 0.28140143  0.02439895  0.16581073]\n",
      " [-0.36319813 -0.3850859  -1.072752  ]]\n",
      "112 0.7054776 [[-0.3961103  -0.40149617  1.3918339 ]\n",
      " [ 0.28120193  0.02602427  0.16438492]\n",
      " [-0.36091533 -0.3857326  -1.0743881 ]]\n",
      "113 0.703773 [[-0.40123424 -0.40333474  1.3987964 ]\n",
      " [ 0.28102332  0.02761299  0.16297483]\n",
      " [-0.35866025 -0.38635415 -1.0760217 ]]\n",
      "114 0.7020891 [[-0.40634236 -0.4051462   1.4057161 ]\n",
      " [ 0.2808647   0.02916625  0.16158022]\n",
      " [-0.35643208 -0.38695145 -1.0776526 ]]\n",
      "115 0.70042527 [[-0.41143474 -0.406931    1.4125932 ]\n",
      " [ 0.28072524  0.03068503  0.16020086]\n",
      " [-0.3542299  -0.38752547 -1.0792807 ]]\n",
      "116 0.6987813 [[-0.41651148 -0.4086895   1.4194285 ]\n",
      " [ 0.2806042   0.03217042  0.15883651]\n",
      " [-0.35205287 -0.38807702 -1.0809062 ]]\n",
      "117 0.69715667 [[-0.4215727  -0.4104221   1.4262222 ]\n",
      " [ 0.2805008   0.0336234   0.15748696]\n",
      " [-0.34990025 -0.38860694 -1.0825288 ]]\n",
      "118 0.695551 [[-0.42661843 -0.41212916  1.432975  ]\n",
      " [ 0.2804143   0.03504486  0.15615198]\n",
      " [-0.3477712  -0.38911608 -1.0841488 ]]\n",
      "119 0.6939639 [[-0.43164882 -0.4138111   1.4396874 ]\n",
      " [ 0.28034398  0.03643582  0.15483135]\n",
      " [-0.345665   -0.38960513 -1.0857658 ]]\n",
      "120 0.69239485 [[-0.43666396 -0.41546825  1.4463596 ]\n",
      " [ 0.2802892   0.03779709  0.15352488]\n",
      " [-0.3435809  -0.39007497 -1.0873802 ]]\n",
      "121 0.6908435 [[-0.44166395 -0.417101    1.4529923 ]\n",
      " [ 0.2802492   0.03912962  0.15223235]\n",
      " [-0.34151825 -0.3905262  -1.0889915 ]]\n",
      "122 0.68930966 [[-0.44664887 -0.4187097   1.4595859 ]\n",
      " [ 0.2802235   0.04043417  0.15095353]\n",
      " [-0.33947626 -0.39095962 -1.0906001 ]]\n",
      "123 0.6877928 [[-0.45161885 -0.4202947   1.4661409 ]\n",
      " [ 0.28021127  0.04171166  0.14968827]\n",
      " [-0.3374544  -0.39137584 -1.0922058 ]]\n",
      "124 0.6862925 [[-0.45657393 -0.42185634  1.4726576 ]\n",
      " [ 0.28021213  0.04296276  0.14843632]\n",
      " [-0.3354519  -0.3917756  -1.0938085 ]]\n",
      "125 0.68480873 [[-0.46151426 -0.42339498  1.4791365 ]\n",
      " [ 0.2802253   0.04418836  0.14719754]\n",
      " [-0.3334683  -0.39215946 -1.0954083 ]]\n",
      "126 0.68334085 [[-0.4664399  -0.42491093  1.4855781 ]\n",
      " [ 0.28025043  0.04538909  0.14597172]\n",
      " [-0.33150283 -0.3925281  -1.0970051 ]]\n",
      "127 0.6818887 [[-0.471351   -0.4264045   1.4919827 ]\n",
      " [ 0.2802868   0.0465658   0.14475864]\n",
      " [-0.32955506 -0.39288196 -1.098599  ]]\n",
      "128 0.680452 [[-0.47624758 -0.42787609  1.4983509 ]\n",
      " [ 0.2803341   0.047719    0.14355814]\n",
      " [-0.32762426 -0.39322186 -1.1001899 ]]\n",
      "129 0.6790303 [[-0.4811298  -0.4293259   1.5046829 ]\n",
      " [ 0.28039154  0.04884958  0.14237007]\n",
      " [-0.32571012 -0.39354807 -1.1017778 ]]\n",
      "130 0.6776234 [[-0.4859977  -0.43075433  1.5109793 ]\n",
      " [ 0.28045896  0.04995801  0.14119422]\n",
      " [-0.3238119  -0.39386135 -1.1033627 ]]\n",
      "131 0.676231 [[-0.49085143 -0.43216166  1.5172404 ]\n",
      " [ 0.28053564  0.05104509  0.14003046]\n",
      " [-0.32192928 -0.39416206 -1.1049446 ]]\n",
      "132 0.6748531 [[-0.49569106 -0.43354818  1.5234666 ]\n",
      " [ 0.28062135  0.05211125  0.1388786 ]\n",
      " [-0.3200616  -0.39445084 -1.1065235 ]]\n",
      "133 0.673489 [[-0.5005167  -0.43491417  1.5296582 ]\n",
      " [ 0.28071544  0.05315727  0.13773848]\n",
      " [-0.31820858 -0.39472798 -1.1080995 ]]\n",
      "134 0.6721386 [[-0.5053284  -0.43625998  1.5358157 ]\n",
      " [ 0.2808177   0.05418352  0.13660999]\n",
      " [-0.31636956 -0.39499414 -1.1096723 ]]\n",
      "135 0.67080176 [[-0.51012635 -0.43758583  1.5419395 ]\n",
      " [ 0.28092754  0.05519076  0.13549289]\n",
      " [-0.3145443  -0.3952496  -1.111242  ]]\n",
      "136 0.66947806 [[-0.5149106  -0.43889204  1.5480299 ]\n",
      " [ 0.28104475  0.05617939  0.13438705]\n",
      " [-0.3127322  -0.3954949  -1.1128088 ]]\n",
      "137 0.6681674 [[-0.5196812  -0.44017884  1.5540873 ]\n",
      " [ 0.28116882  0.05715003  0.13329235]\n",
      " [-0.310933   -0.39573038 -1.1143726 ]]\n",
      "138 0.6668695 [[-0.5244383  -0.44144654  1.5601121 ]\n",
      " [ 0.28129953  0.05810304  0.13220862]\n",
      " [-0.30914614 -0.39595655 -1.1159333 ]]\n",
      "139 0.6655841 [[-0.529182   -0.44269538  1.5661047 ]\n",
      " [ 0.2814364   0.05903908  0.13113569]\n",
      " [-0.3073714  -0.3961736  -1.117491  ]]\n",
      "140 0.66431093 [[-0.53391236 -0.44392565  1.5720652 ]\n",
      " [ 0.28157926  0.05995846  0.13007346]\n",
      " [-0.3056083  -0.39638212 -1.1190456 ]]\n",
      "141 0.66305006 [[-0.5386295  -0.44513756  1.5779942 ]\n",
      " [ 0.28172758  0.06086182  0.12902176]\n",
      " [-0.3038566  -0.39658228 -1.1205971 ]]\n",
      "142 0.66180086 [[-0.5433334  -0.4463314   1.583892  ]\n",
      " [ 0.2818813   0.0617494   0.12798047]\n",
      " [-0.3021158  -0.3967746  -1.1221457 ]]\n",
      "143 0.6605634 [[-0.54802436 -0.4475074   1.5897589 ]\n",
      " [ 0.2820399   0.06262185  0.12694943]\n",
      " [-0.3003857  -0.3969592  -1.1236911 ]]\n",
      "144 0.6593374 [[-0.5527023  -0.44866586  1.5955952 ]\n",
      " [ 0.28220332  0.06347935  0.12592854]\n",
      " [-0.29866588 -0.39713663 -1.1252335 ]]\n",
      "145 0.6581228 [[-0.5573674  -0.44980693  1.6014014 ]\n",
      " [ 0.28237104  0.06432253  0.12491764]\n",
      " [-0.29695612 -0.39730698 -1.1267729 ]]\n",
      "146 0.6569191 [[-0.5620197  -0.4509309   1.6071777 ]\n",
      " [ 0.282543    0.0651516   0.12391661]\n",
      " [-0.29525602 -0.39747074 -1.1283092 ]]\n",
      "147 0.65572643 [[-0.56665933 -0.45203793  1.6129245 ]\n",
      " [ 0.28271884  0.06596708  0.12292533]\n",
      " [-0.2935654  -0.39762807 -1.1298425 ]]\n",
      "148 0.6545445 [[-0.5712864  -0.4531283   1.6186419 ]\n",
      " [ 0.2828984   0.06676921  0.12194365]\n",
      " [-0.29188386 -0.39777932 -1.1313728 ]]\n",
      "149 0.65337294 [[-0.575901   -0.45420226  1.6243304 ]\n",
      " [ 0.28308135  0.06755842  0.12097149]\n",
      " [-0.29021123 -0.3979247  -1.1329001 ]]\n",
      "150 0.6522119 [[-0.5805031  -0.45525998  1.6299903 ]\n",
      " [ 0.28326756  0.06833501  0.12000873]\n",
      " [-0.2885472  -0.39806452 -1.1344243 ]]\n",
      "151 0.65106094 [[-0.58509296 -0.4563017   1.6356219 ]\n",
      " [ 0.2834567   0.06909938  0.11905521]\n",
      " [-0.28689152 -0.39819896 -1.1359456 ]]\n",
      "152 0.64992005 [[-0.5896706  -0.4573276   1.6412255 ]\n",
      " [ 0.2836487   0.06985177  0.11811084]\n",
      " [-0.28524393 -0.39832833 -1.1374638 ]]\n",
      "153 0.64878905 [[-0.5942361  -0.45833793  1.6468012 ]\n",
      " [ 0.28384325  0.07059255  0.11717551]\n",
      " [-0.28360423 -0.39845282 -1.138979  ]]\n",
      "154 0.6476678 [[-0.5987895  -0.45933285  1.6523496 ]\n",
      " [ 0.2840402   0.07132202  0.11624908]\n",
      " [-0.2819722  -0.39857262 -1.1404911 ]]\n",
      "155 0.646556 [[-0.60333097 -0.4603126   1.6578708 ]\n",
      " [ 0.28423944  0.0720404   0.11533146]\n",
      " [-0.28034756 -0.39868808 -1.1420003 ]]\n",
      "156 0.6454537 [[-0.60786057 -0.46127737  1.6633651 ]\n",
      " [ 0.28444067  0.07274812  0.11442253]\n",
      " [-0.27873018 -0.39879918 -1.1435065 ]]\n",
      "157 0.64436066 [[-0.61237836 -0.46222734  1.6688329 ]\n",
      " [ 0.28464386  0.07344528  0.1135222 ]\n",
      " [-0.27711973 -0.39890635 -1.1450098 ]]\n",
      "158 0.6432767 [[-0.6168845  -0.4631627   1.6742744 ]\n",
      " [ 0.28484863  0.07413234  0.11263036]\n",
      " [-0.2755162  -0.39900956 -1.14651   ]]\n",
      "159 0.64220166 [[-0.621379   -0.46408364  1.6796899 ]\n",
      " [ 0.28505513  0.0748093   0.11174691]\n",
      " [-0.27391917 -0.39910924 -1.1480074 ]]\n",
      "160 0.6411356 [[-0.62586206 -0.46499035  1.6850796 ]\n",
      " [ 0.28526288  0.07547671  0.11087175]\n",
      " [-0.27232873 -0.3992053  -1.1495018 ]]\n",
      "161 0.64007807 [[-0.6303336  -0.46588302  1.6904438 ]\n",
      " [ 0.28547204  0.07613452  0.1100048 ]\n",
      " [-0.2707444  -0.3992982  -1.1509932 ]]\n",
      "162 0.63902915 [[-0.6347938  -0.4667618   1.6957828 ]\n",
      " [ 0.2856822   0.07678324  0.10914592]\n",
      " [-0.26916632 -0.3993878  -1.1524817 ]]\n",
      "163 0.6379886 [[-0.6392427  -0.4676269   1.7010968 ]\n",
      " [ 0.28589353  0.07742283  0.108295  ]\n",
      " [-0.26759398 -0.39947456 -1.1539673 ]]\n",
      "164 0.63695645 [[-0.64368045 -0.46847844  1.7063861 ]\n",
      " [ 0.28610557  0.0780538   0.107452  ]\n",
      " [-0.26602754 -0.39955834 -1.1554499 ]]\n",
      "165 0.63593245 [[-0.64810705 -0.46931663  1.711651  ]\n",
      " [ 0.2863185   0.0786761   0.10661678]\n",
      " [-0.2644666  -0.3996395  -1.1569296 ]]\n",
      "166 0.6349165 [[-0.6525227  -0.47014165  1.7168916 ]\n",
      " [ 0.28653204  0.07929011  0.10578927]\n",
      " [-0.26291117 -0.39971805 -1.1584065 ]]\n",
      "167 0.6339085 [[-0.65692735 -0.47095364  1.7221082 ]\n",
      " [ 0.28674608  0.07989594  0.1049694 ]\n",
      " [-0.26136106 -0.3997942  -1.1598804 ]]\n",
      "168 0.6329082 [[-0.66132116 -0.47175276  1.7273011 ]\n",
      " [ 0.2869606   0.08049379  0.10415704]\n",
      " [-0.25981608 -0.39986807 -1.1613514 ]]\n",
      "169 0.6319158 [[-0.6657042  -0.4725392   1.7324705 ]\n",
      " [ 0.28717542  0.08108392  0.1033521 ]\n",
      " [-0.25827616 -0.39993975 -1.1628196 ]]\n",
      "170 0.6309309 [[-0.6700765  -0.47331306  1.7376167 ]\n",
      " [ 0.2873905   0.08166641  0.10255452]\n",
      " [-0.2567411  -0.40000945 -1.164285  ]]\n",
      "171 0.6299534 [[-0.6744382  -0.4740745   1.7427398 ]\n",
      " [ 0.2876056   0.08224159  0.1017642 ]\n",
      " [-0.25521094 -0.4000771  -1.1657475 ]]\n",
      "172 0.6289834 [[-0.6787893  -0.47482374  1.7478402 ]\n",
      " [ 0.28782094  0.0828094   0.1009811 ]\n",
      " [-0.2536853  -0.4001431  -1.1672071 ]]\n",
      "173 0.6280206 [[-0.68313    -0.47556087  1.752918  ]\n",
      " [ 0.28803608  0.08337028  0.10020508]\n",
      " [-0.2521643  -0.40020722 -1.168664  ]]\n",
      "174 0.62706494 [[-0.6874603  -0.47628605  1.7579734 ]\n",
      " [ 0.28825122  0.08392415  0.09943607]\n",
      " [-0.25064763 -0.40026987 -1.170118  ]]\n",
      "175 0.6261164 [[-0.69178027 -0.47699943  1.7630068 ]\n",
      " [ 0.28846607  0.0844714   0.098674  ]\n",
      " [-0.24913539 -0.4003309  -1.1715692 ]]\n",
      "176 0.62517476 [[-0.69609    -0.47770116  1.7680182 ]\n",
      " [ 0.28868073  0.08501194  0.09791879]\n",
      " [-0.24762727 -0.40039062 -1.1730176 ]]\n",
      "177 0.62424004 [[-0.70038956 -0.47839135  1.773008  ]\n",
      " [ 0.28889498  0.08554611  0.09717035]\n",
      " [-0.24612334 -0.40044895 -1.1744633 ]]\n",
      "178 0.62331206 [[-0.704679   -0.4790702   1.7779763 ]\n",
      " [ 0.2891089   0.08607391  0.09642863]\n",
      " [-0.24462335 -0.4005061  -1.1759061 ]]\n",
      "179 0.6223907 [[-0.70895845 -0.47973773  1.7829233 ]\n",
      " [ 0.2893223   0.08659565  0.09569351]\n",
      " [-0.24312735 -0.40056205 -1.1773461 ]]\n",
      "180 0.62147593 [[-0.7132279  -0.48039418  1.7878493 ]\n",
      " [ 0.28953522  0.08711129  0.09496494]\n",
      " [-0.24163508 -0.40061697 -1.1787834 ]]\n",
      "181 0.6205677 [[-0.7174876  -0.48103964  1.7927544 ]\n",
      " [ 0.28974754  0.08762109  0.09424284]\n",
      " [-0.24014659 -0.40067086 -1.180218  ]]\n",
      "182 0.61966586 [[-0.7217374  -0.48167425  1.7976389 ]\n",
      " [ 0.28995922  0.08812509  0.09352715]\n",
      " [-0.23866175 -0.40072387 -1.1816498 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 0.61877024 [[-0.7259775  -0.48229814  1.8025029 ]\n",
      " [ 0.2901702   0.08862347  0.09281779]\n",
      " [-0.23718047 -0.40077603 -1.1830789 ]]\n",
      "184 0.61788094 [[-0.7302079  -0.4829114   1.8073466 ]\n",
      " [ 0.29038042  0.08911635  0.09211469]\n",
      " [-0.23570268 -0.4008274  -1.1845053 ]]\n",
      "185 0.6169977 [[-0.73442876 -0.48351422  1.8121703 ]\n",
      " [ 0.2905899   0.08960377  0.0914178 ]\n",
      " [-0.23422827 -0.40087813 -1.1859291 ]]\n",
      "186 0.6161206 [[-0.7386401  -0.48410666  1.816974  ]\n",
      " [ 0.2907985   0.09008598  0.09072702]\n",
      " [-0.23275726 -0.40092814 -1.18735   ]]\n",
      "187 0.6152494 [[-0.742842   -0.48468888  1.8217582 ]\n",
      " [ 0.29100627  0.09056293  0.09004229]\n",
      " [-0.23128942 -0.40097764 -1.1887684 ]]\n",
      "188 0.6143842 [[-0.74703455 -0.48526096  1.8265227 ]\n",
      " [ 0.29121307  0.0910349   0.08936354]\n",
      " [-0.22982486 -0.40102655 -1.190184  ]]\n",
      "189 0.6135247 [[-0.7512177  -0.48582307  1.831268  ]\n",
      " [ 0.29141897  0.09150183  0.08869071]\n",
      " [-0.22836335 -0.40107507 -1.191597  ]]\n",
      "190 0.612671 [[-0.75539166 -0.48637527  1.8359941 ]\n",
      " [ 0.29162383  0.09196396  0.08802374]\n",
      " [-0.22690496 -0.4011231  -1.1930073 ]]\n",
      "191 0.61182296 [[-0.7595564  -0.48691773  1.8407013 ]\n",
      " [ 0.2918277   0.09242129  0.08736257]\n",
      " [-0.22544953 -0.40117085 -1.1944151 ]]\n",
      "192 0.6109805 [[-0.76371205 -0.4874505   1.8453897 ]\n",
      " [ 0.29203045  0.09287399  0.08670712]\n",
      " [-0.2239971  -0.40121824 -1.1958201 ]]\n",
      "193 0.61014354 [[-0.7678586  -0.48797375  1.8500595 ]\n",
      " [ 0.2922322   0.09332206  0.08605731]\n",
      " [-0.22254747 -0.4012654  -1.1972226 ]]\n",
      "194 0.60931206 [[-0.77199626 -0.48848754  1.8547109 ]\n",
      " [ 0.29243273  0.09376569  0.08541312]\n",
      " [-0.22110072 -0.40131235 -1.1986225 ]]\n",
      "195 0.608486 [[-0.77612495 -0.488992    1.8593441 ]\n",
      " [ 0.29263216  0.0942049   0.08477448]\n",
      " [-0.2196567  -0.40135914 -1.2000197 ]]\n",
      "196 0.6076652 [[-0.78024477 -0.48948723  1.8639592 ]\n",
      " [ 0.2928304   0.09463984  0.08414131]\n",
      " [-0.21821542 -0.40140578 -1.2014143 ]]\n",
      "197 0.60684955 [[-0.78435576 -0.48997334  1.8685563 ]\n",
      " [ 0.2930275   0.09507051  0.08351354]\n",
      " [-0.21677677 -0.40145236 -1.2028065 ]]\n",
      "198 0.60603917 [[-0.78845805 -0.4904504   1.8731356 ]\n",
      " [ 0.2932233   0.09549707  0.08289116]\n",
      " [-0.2153408  -0.40149885 -1.204196  ]]\n",
      "199 0.60523385 [[-0.79255164 -0.49091858  1.8776973 ]\n",
      " [ 0.29341793  0.09591953  0.08227406]\n",
      " [-0.21390733 -0.40154538 -1.205583  ]]\n",
      "200 0.6044336 [[-0.79663664 -0.4913779   1.8822417 ]\n",
      " [ 0.29361123  0.09633806  0.08166222]\n",
      " [-0.21247645 -0.40159187 -1.2069674 ]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#lab07-01\n",
    "'''\n",
    "learning rate: gradient descent optimizer에서 step을 얼마로 설정해줄 것인지 정하는 것\n",
    "overshooting: learning rate가 너무 클때는 한 번에 step이 너무 커짐, 밖으로 빠져나갈지도\n",
    "small learning rate: 너무 조금씩 step을 줌, 산을 타고 내려갈 때 조금조금 내려가면 해질때까지도 못내려가는 것처럼\n",
    "learning rate를 너무 적게 주면 어느정도 내려가다가 멈추면 그게 최저가 아님에도 불구하고 최저라고 판단함.\n",
    "learning rate에 너무 조금씩 step을 줘서 cost가 너무 작은 값으로 변한다 싶으면 좀 높여줘야 함.\n",
    "'''\n",
    "#tf.square(W): regularization하겠다.(overfitting을 방지하기 위함)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "#training set\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "#y_data는 one_hot으로 주어져있음\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#test set\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder('float', [None, 3])\n",
    "Y = tf.placeholder('float', [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], \n",
    "                        feed_dict = {X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict = {X:x_test}))\n",
    "    print('Accuracy: ', sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.4381795 [[-1.5684046  -0.47728956 -1.35927   ]\n",
      " [-2.8842747   1.743624    0.78208756]\n",
      " [-4.34633     1.1646693  -0.20346165]]\n",
      "1 18.240692 [[-1.1934046  -1.37761    -0.83394945]\n",
      " [-0.44677472 -2.1154163   2.2036278 ]\n",
      " [-1.9088302  -2.5428674   1.0665748 ]]\n",
      "2 25.93694 [[-0.818407   -0.81510997 -1.7714471 ]\n",
      " [ 1.9907203   0.5095837  -2.8588674 ]\n",
      " [ 0.52866745  0.26963258 -4.183423  ]]\n",
      "3 12.384337 [[-1.9398863  -0.25613046 -1.2089472 ]\n",
      " [-2.1265965   3.1269007  -1.3588676 ]\n",
      " [-3.5886488   3.0744486  -2.8709233 ]]\n",
      "4 28.981215 [[-1.5648863  -1.1936301  -0.6464475 ]\n",
      " [ 0.31090355 -0.8105986   0.14113164]\n",
      " [-1.1511488  -0.67555094 -1.5584236 ]]\n",
      "5 2.1016626 [[-1.5087432  -0.6558883  -1.2403324 ]\n",
      " [ 0.96140885  1.7511358  -3.0711076 ]\n",
      " [-0.4729234   2.0187256  -4.9309254 ]]\n",
      "6 15.473864 [[-1.13582   -1.5913115 -0.6778325]\n",
      " [ 3.3946784 -2.1821337 -1.5711079]\n",
      " [ 1.9624218 -1.7291193 -3.6184254]]\n",
      "7 26.147543 [[-2.2608192  -1.0288115  -0.11533332]\n",
      " [-0.7303202   0.44286633 -0.07110953]\n",
      " [-2.1625772   1.0833807  -2.3059263 ]]\n",
      "8 9.126955 [[-1.8858806  -1.8232267   0.30414325]\n",
      " [ 1.7070546  -3.1994247   1.1338067 ]\n",
      " [ 0.2748592  -2.514471   -1.1455108 ]]\n",
      "9 14.426041 [[-2.55902    -1.2607267   0.4147827 ]\n",
      " [-1.2533495  -0.57442474  1.469211  ]\n",
      " [-2.887061    0.29802895 -0.79609025]]\n",
      "10 13.97246 [[-2.18402    -0.69905794 -0.521886  ]\n",
      " [ 1.1841505   2.0488658  -3.5915794 ]\n",
      " [-0.44956112  3.1064565  -6.0420175 ]]\n",
      "11 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "12 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "13 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "14 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "15 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "16 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "17 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "18 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "19 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "21 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "22 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "23 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "24 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "25 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "26 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "27 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "28 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "29 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "30 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "31 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "32 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "33 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "34 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "35 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "36 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "37 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "38 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "39 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "41 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "42 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "43 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "44 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "45 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "46 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "47 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "48 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "49 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "50 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "51 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "52 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "53 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "54 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "55 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "56 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "57 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "58 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "59 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "61 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "62 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "63 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "64 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "65 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "66 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "67 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "68 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "69 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "70 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "71 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "72 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "73 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "74 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "75 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "76 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "77 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "78 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "79 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "81 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "82 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "83 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "84 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "85 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "86 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "87 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "88 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "89 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "90 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "91 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "92 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "93 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "94 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "95 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "96 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "97 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "98 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "99 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "101 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "102 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "103 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "104 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "105 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "106 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "107 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "108 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "109 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "110 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "111 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "112 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "113 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "114 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "115 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "116 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "117 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "118 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "119 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "121 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "122 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "123 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "124 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "125 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "126 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "127 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "128 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "129 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "130 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "131 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "132 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "133 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "134 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "135 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "136 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "137 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "138 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "139 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "141 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "142 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "143 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "144 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "145 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "146 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "147 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "148 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "149 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "150 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "151 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "152 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "153 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "154 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "156 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "157 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "158 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "159 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "161 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "162 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "163 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "164 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "165 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "166 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "167 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "168 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "169 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "170 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "171 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "172 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "173 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "174 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "175 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "176 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "177 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "178 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "179 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "181 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "182 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "183 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "184 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "185 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "186 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "187 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "188 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "189 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "190 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "191 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "192 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "193 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "194 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "195 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "196 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "197 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "198 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "199 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction:  [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "#밖으로 튕겨나가는 경우, 학습을 포기\n",
    "#cost에 nan이 나오면 learning_rate가 너무 큰 것은 아닌지 봐야 함.\n",
    "#training set\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "#y_data는 one_hot으로 주어져있음\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#test set\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder('float', [None, 3])\n",
    "Y = tf.placeholder('float', [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1.5).minimize(cost)\n",
    "\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], \n",
    "                                      feed_dict = {X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "    \n",
    "    print('Prediction: ', sess.run(prediction, feed_dict = {X: x_test}))\n",
    "    print('Accuracy: ', sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "1 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "2 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "3 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "4 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "5 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "6 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "7 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "8 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "9 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "10 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "11 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "12 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "13 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "14 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "15 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "16 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "17 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "18 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "19 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "20 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "21 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "22 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "23 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "24 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "25 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "26 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "27 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "28 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "29 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "30 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "31 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "32 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "33 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "34 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "35 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "36 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "37 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "38 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "39 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "40 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "41 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "42 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "43 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "44 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "45 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "46 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "47 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "48 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "49 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "50 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "51 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "52 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "53 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "54 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "55 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "56 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "57 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "58 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "59 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "60 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "61 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "62 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "63 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "64 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "65 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "66 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "67 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "68 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "69 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "70 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "71 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "72 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "73 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "74 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "75 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "76 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "77 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "78 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "79 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "80 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "81 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "82 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "83 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "84 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "85 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "86 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "88 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "89 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "90 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "91 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "92 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "93 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "94 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "95 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "96 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "97 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "98 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "99 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "100 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "101 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "102 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "103 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "104 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "105 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "106 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "107 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "108 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "109 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "110 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "111 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "112 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "113 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "114 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "115 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "116 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "117 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "118 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "119 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "120 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "121 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "122 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "123 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "124 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "125 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "126 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "127 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "128 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "129 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "130 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "131 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "132 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "133 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "134 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "135 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "136 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "137 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "138 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "139 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "140 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "141 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "142 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "143 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "144 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "145 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "146 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "147 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "148 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "149 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "150 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "151 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "152 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "153 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "154 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "155 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "156 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "157 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "158 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "159 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "160 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "161 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "162 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "163 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "164 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "166 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "167 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "168 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "169 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "170 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "171 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "172 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "173 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "174 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "175 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "176 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "177 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "178 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "179 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "180 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "181 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "182 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "183 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "184 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "185 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "186 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "187 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "188 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "189 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "190 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "191 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "192 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "193 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "194 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "195 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "196 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "197 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "198 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "199 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "200 3.9474359 [[-0.38523248 -0.0960717  -1.6764985 ]\n",
      " [ 0.44569156 -1.0694239  -1.1472123 ]\n",
      " [ 0.57059914  0.51232064  0.11380499]]\n",
      "Prediction:  [1 1 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "#small learning rate인 경우 local minima에 빠져서 나오지 못함.\n",
    "#training set\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "#y_data는 one_hot으로 주어져있음\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "#test set\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder('float', [None, 3])\n",
    "Y = tf.placeholder('float', [None, 3])\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-10).minimize(cost)\n",
    "\n",
    "prediction = tf.arg_max(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict = {X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    print('Prediction: ', sess.run(prediction, feed_dict = {X: x_test}))\n",
    "    print('Accuracy: ', sess.run(accuracy, feed_dict= {X: x_test, Y: y_test}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  2696877700000.0 \n",
      "Prediction:\n",
      " [[1158700.2]\n",
      " [2333457.5]\n",
      " [1835468.6]\n",
      " [1286403.6]\n",
      " [1516243.8]\n",
      " [1529014.8]\n",
      " [1401331.2]\n",
      " [1784405.1]]\n",
      "1 Cost:  2.9630066e+27 \n",
      "Prediction:\n",
      " [[-3.8397024e+13]\n",
      " [-7.7297156e+13]\n",
      " [-6.0806884e+13]\n",
      " [-4.2625302e+13]\n",
      " [-5.0236194e+13]\n",
      " [-5.0659022e+13]\n",
      " [-4.6430748e+13]\n",
      " [-5.9115573e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[1.2727211e+21]\n",
      " [2.5621184e+21]\n",
      " [2.0155259e+21]\n",
      " [1.4128729e+21]\n",
      " [1.6651463e+21]\n",
      " [1.6791615e+21]\n",
      " [1.5390096e+21]\n",
      " [1.9594653e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[-4.2186061e+28]\n",
      " [-8.4924876e+28]\n",
      " [-6.6807328e+28]\n",
      " [-4.6831581e+28]\n",
      " [-5.5193522e+28]\n",
      " [-5.5658076e+28]\n",
      " [-5.1012552e+28]\n",
      " [-6.4949119e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[1.3983138e+36]\n",
      " [2.8149490e+36]\n",
      " [2.2144191e+36]\n",
      " [1.5522960e+36]\n",
      " [1.8294638e+36]\n",
      " [1.8448620e+36]\n",
      " [1.6908798e+36]\n",
      " [2.1528261e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "#MinMaxScalar함수: 정규화할 수 있음(min: 0, max: 1)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_daya = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder('float', [None, 4])\n",
    "Y = tf.placeholder('float', [None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(101):\n",
    "        cost_val, hy_val, _ = sess.run(\n",
    "            [cost, hypothesis, optimizer], feed_dict = {X: x_data, Y: y_data})\n",
    "        print(step, \"Cost: \", cost_val, '\\nPrediction:\\n', hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  9.413631 \n",
      "Prediction:\n",
      " [[3.8384354]\n",
      " [3.6957006]\n",
      " [3.5720372]\n",
      " [3.4605544]\n",
      " [3.5276406]\n",
      " [3.6094432]\n",
      " [3.3297431]\n",
      " [3.5008051]]\n",
      "1 Cost:  9.41297 \n",
      "Prediction:\n",
      " [[3.838291 ]\n",
      " [3.695557 ]\n",
      " [3.5719166]\n",
      " [3.4604595]\n",
      " [3.5275295]\n",
      " [3.6093357]\n",
      " [3.329667 ]\n",
      " [3.5007296]]\n",
      "2 Cost:  9.41231 \n",
      "Prediction:\n",
      " [[3.8381467]\n",
      " [3.6954138]\n",
      " [3.5717957]\n",
      " [3.4603646]\n",
      " [3.5274186]\n",
      " [3.6092281]\n",
      " [3.3295913]\n",
      " [3.5006537]]\n",
      "3 Cost:  9.41165 \n",
      "Prediction:\n",
      " [[3.8380024]\n",
      " [3.6952703]\n",
      " [3.571675 ]\n",
      " [3.4602695]\n",
      " [3.5273077]\n",
      " [3.6091206]\n",
      " [3.3295155]\n",
      " [3.5005782]]\n",
      "4 Cost:  9.41099 \n",
      "Prediction:\n",
      " [[3.8378582]\n",
      " [3.6951268]\n",
      " [3.5715542]\n",
      " [3.4601746]\n",
      " [3.527197 ]\n",
      " [3.609013 ]\n",
      " [3.3294396]\n",
      " [3.5005026]]\n",
      "5 Cost:  9.41033 \n",
      "Prediction:\n",
      " [[3.8377137]\n",
      " [3.6949832]\n",
      " [3.5714338]\n",
      " [3.4600797]\n",
      " [3.5270858]\n",
      " [3.6089056]\n",
      " [3.3293638]\n",
      " [3.500427 ]]\n",
      "6 Cost:  9.40967 \n",
      "Prediction:\n",
      " [[3.8375695]\n",
      " [3.69484  ]\n",
      " [3.571313 ]\n",
      " [3.4599848]\n",
      " [3.526975 ]\n",
      " [3.608798 ]\n",
      " [3.329288 ]\n",
      " [3.5003514]]\n",
      "7 Cost:  9.40901 \n",
      "Prediction:\n",
      " [[3.8374252]\n",
      " [3.6946964]\n",
      " [3.5711923]\n",
      " [3.4598897]\n",
      " [3.526864 ]\n",
      " [3.6086905]\n",
      " [3.3292122]\n",
      " [3.5002756]]\n",
      "8 Cost:  9.40835 \n",
      "Prediction:\n",
      " [[3.837281 ]\n",
      " [3.694553 ]\n",
      " [3.5710716]\n",
      " [3.4597948]\n",
      " [3.526753 ]\n",
      " [3.608583 ]\n",
      " [3.3291364]\n",
      " [3.5002   ]]\n",
      "9 Cost:  9.407691 \n",
      "Prediction:\n",
      " [[3.8371367]\n",
      " [3.6944096]\n",
      " [3.570951 ]\n",
      " [3.4596999]\n",
      " [3.5266423]\n",
      " [3.6084754]\n",
      " [3.3290606]\n",
      " [3.5001245]]\n",
      "10 Cost:  9.407031 \n",
      "Prediction:\n",
      " [[3.8369923]\n",
      " [3.694266 ]\n",
      " [3.5708303]\n",
      " [3.459605 ]\n",
      " [3.5265312]\n",
      " [3.608368 ]\n",
      " [3.3289845]\n",
      " [3.5000489]]\n",
      "11 Cost:  9.406371 \n",
      "Prediction:\n",
      " [[3.8368483]\n",
      " [3.6941228]\n",
      " [3.5707097]\n",
      " [3.45951  ]\n",
      " [3.5264204]\n",
      " [3.6082604]\n",
      " [3.3289087]\n",
      " [3.499973 ]]\n",
      "12 Cost:  9.405712 \n",
      "Prediction:\n",
      " [[3.836704 ]\n",
      " [3.6939793]\n",
      " [3.570589 ]\n",
      " [3.4594152]\n",
      " [3.5263095]\n",
      " [3.6081529]\n",
      " [3.3288329]\n",
      " [3.4998975]]\n",
      "13 Cost:  9.405052 \n",
      "Prediction:\n",
      " [[3.8365598]\n",
      " [3.6938357]\n",
      " [3.5704684]\n",
      " [3.4593203]\n",
      " [3.5261986]\n",
      " [3.6080453]\n",
      " [3.328757 ]\n",
      " [3.499822 ]]\n",
      "14 Cost:  9.404392 \n",
      "Prediction:\n",
      " [[3.8364153]\n",
      " [3.6936924]\n",
      " [3.5703478]\n",
      " [3.4592254]\n",
      " [3.5260878]\n",
      " [3.6079378]\n",
      " [3.3286812]\n",
      " [3.4997463]]\n",
      "15 Cost:  9.403732 \n",
      "Prediction:\n",
      " [[3.8362713]\n",
      " [3.693549 ]\n",
      " [3.5702271]\n",
      " [3.4591305]\n",
      " [3.5259767]\n",
      " [3.6078303]\n",
      " [3.3286054]\n",
      " [3.4996707]]\n",
      "16 Cost:  9.403073 \n",
      "Prediction:\n",
      " [[3.836127 ]\n",
      " [3.6934054]\n",
      " [3.5701065]\n",
      " [3.4590354]\n",
      " [3.5258658]\n",
      " [3.6077228]\n",
      " [3.3285296]\n",
      " [3.499595 ]]\n",
      "17 Cost:  9.402414 \n",
      "Prediction:\n",
      " [[3.8359828]\n",
      " [3.693262 ]\n",
      " [3.5699859]\n",
      " [3.4589405]\n",
      " [3.525755 ]\n",
      " [3.6076152]\n",
      " [3.3284538]\n",
      " [3.4995193]]\n",
      "18 Cost:  9.401754 \n",
      "Prediction:\n",
      " [[3.8358388]\n",
      " [3.6931188]\n",
      " [3.5698652]\n",
      " [3.4588456]\n",
      " [3.525644 ]\n",
      " [3.6075077]\n",
      " [3.328378 ]\n",
      " [3.4994438]]\n",
      "19 Cost:  9.401095 \n",
      "Prediction:\n",
      " [[3.8356943]\n",
      " [3.6929755]\n",
      " [3.5697448]\n",
      " [3.4587507]\n",
      " [3.5255332]\n",
      " [3.6074002]\n",
      " [3.3283021]\n",
      " [3.4993682]]\n",
      "20 Cost:  9.400436 \n",
      "Prediction:\n",
      " [[3.8355503]\n",
      " [3.692832 ]\n",
      " [3.5696242]\n",
      " [3.4586558]\n",
      " [3.5254223]\n",
      " [3.607293 ]\n",
      " [3.3282263]\n",
      " [3.4992926]]\n",
      "21 Cost:  9.399776 \n",
      "Prediction:\n",
      " [[3.835406 ]\n",
      " [3.6926885]\n",
      " [3.5695035]\n",
      " [3.458561 ]\n",
      " [3.5253115]\n",
      " [3.6071854]\n",
      " [3.3281505]\n",
      " [3.4992168]]\n",
      "22 Cost:  9.399118 \n",
      "Prediction:\n",
      " [[3.8352618]\n",
      " [3.6925452]\n",
      " [3.569383 ]\n",
      " [3.458466 ]\n",
      " [3.5252006]\n",
      " [3.6070778]\n",
      " [3.3280747]\n",
      " [3.4991412]]\n",
      "23 Cost:  9.3984585 \n",
      "Prediction:\n",
      " [[3.8351176]\n",
      " [3.692402 ]\n",
      " [3.5692625]\n",
      " [3.4583712]\n",
      " [3.5250897]\n",
      " [3.6069703]\n",
      " [3.3279989]\n",
      " [3.4990656]]\n",
      "24 Cost:  9.3977995 \n",
      "Prediction:\n",
      " [[3.8349736]\n",
      " [3.6922584]\n",
      " [3.5691419]\n",
      " [3.4582763]\n",
      " [3.5249789]\n",
      " [3.606863 ]\n",
      " [3.327923 ]\n",
      " [3.49899  ]]\n",
      "25 Cost:  9.3971405 \n",
      "Prediction:\n",
      " [[3.8348293]\n",
      " [3.692115 ]\n",
      " [3.5690212]\n",
      " [3.4581814]\n",
      " [3.524868 ]\n",
      " [3.6067555]\n",
      " [3.3278472]\n",
      " [3.4989145]]\n",
      "26 Cost:  9.3964815 \n",
      "Prediction:\n",
      " [[3.8346853]\n",
      " [3.6919718]\n",
      " [3.5689006]\n",
      " [3.4580865]\n",
      " [3.5247571]\n",
      " [3.606648 ]\n",
      " [3.3277712]\n",
      " [3.498839 ]]\n",
      "27 Cost:  9.395823 \n",
      "Prediction:\n",
      " [[3.8345408]\n",
      " [3.6918283]\n",
      " [3.56878  ]\n",
      " [3.4579916]\n",
      " [3.5246463]\n",
      " [3.6065404]\n",
      " [3.3276954]\n",
      " [3.498763 ]]\n",
      "28 Cost:  9.395163 \n",
      "Prediction:\n",
      " [[3.8343968]\n",
      " [3.691685 ]\n",
      " [3.5686593]\n",
      " [3.4578967]\n",
      " [3.5245354]\n",
      " [3.606433 ]\n",
      " [3.3276196]\n",
      " [3.4986875]]\n",
      "29 Cost:  9.394505 \n",
      "Prediction:\n",
      " [[3.8342526]\n",
      " [3.6915417]\n",
      " [3.568539 ]\n",
      " [3.4578018]\n",
      " [3.5244246]\n",
      " [3.6063254]\n",
      " [3.3275437]\n",
      " [3.498612 ]]\n",
      "30 Cost:  9.393845 \n",
      "Prediction:\n",
      " [[3.8341084]\n",
      " [3.6913981]\n",
      " [3.5684183]\n",
      " [3.457707 ]\n",
      " [3.5243137]\n",
      " [3.6062179]\n",
      " [3.327468 ]\n",
      " [3.4985363]]\n",
      "31 Cost:  9.393187 \n",
      "Prediction:\n",
      " [[3.833964 ]\n",
      " [3.6912549]\n",
      " [3.5682976]\n",
      " [3.457612 ]\n",
      " [3.5242028]\n",
      " [3.6061106]\n",
      " [3.327392 ]\n",
      " [3.4984608]]\n",
      "32 Cost:  9.392528 \n",
      "Prediction:\n",
      " [[3.83382  ]\n",
      " [3.6911116]\n",
      " [3.568177 ]\n",
      " [3.4575171]\n",
      " [3.524092 ]\n",
      " [3.606003 ]\n",
      " [3.3273163]\n",
      " [3.498385 ]]\n",
      "33 Cost:  9.391869 \n",
      "Prediction:\n",
      " [[3.8336759]\n",
      " [3.690968 ]\n",
      " [3.5680563]\n",
      " [3.4574223]\n",
      " [3.523981 ]\n",
      " [3.6058955]\n",
      " [3.3272405]\n",
      " [3.4983094]]\n",
      "34 Cost:  9.39121 \n",
      "Prediction:\n",
      " [[3.8335319]\n",
      " [3.6908247]\n",
      " [3.567936 ]\n",
      " [3.4573274]\n",
      " [3.5238702]\n",
      " [3.6057882]\n",
      " [3.3271646]\n",
      " [3.4982338]]\n",
      "35 Cost:  9.390552 \n",
      "Prediction:\n",
      " [[3.8333876]\n",
      " [3.6906815]\n",
      " [3.5678153]\n",
      " [3.4572325]\n",
      " [3.5237594]\n",
      " [3.6056807]\n",
      " [3.3270888]\n",
      " [3.4981582]]\n",
      "36 Cost:  9.389893 \n",
      "Prediction:\n",
      " [[3.8332434]\n",
      " [3.6905382]\n",
      " [3.5676947]\n",
      " [3.4571376]\n",
      " [3.5236487]\n",
      " [3.6055732]\n",
      " [3.327013 ]\n",
      " [3.4980826]]\n",
      "37 Cost:  9.389234 \n",
      "Prediction:\n",
      " [[3.8330994]\n",
      " [3.6903949]\n",
      " [3.567574 ]\n",
      " [3.4570427]\n",
      " [3.5235376]\n",
      " [3.6054657]\n",
      " [3.3269372]\n",
      " [3.498007 ]]\n",
      "38 Cost:  9.3885765 \n",
      "Prediction:\n",
      " [[3.8329554]\n",
      " [3.6902516]\n",
      " [3.5674539]\n",
      " [3.456948 ]\n",
      " [3.5234272]\n",
      " [3.6053586]\n",
      " [3.3268616]\n",
      " [3.4979317]]\n",
      "39 Cost:  9.387919 \n",
      "Prediction:\n",
      " [[3.8328116]\n",
      " [3.6901088]\n",
      " [3.5673335]\n",
      " [3.4568534]\n",
      " [3.5233164]\n",
      " [3.6052513]\n",
      " [3.326786 ]\n",
      " [3.4978561]]\n",
      "40 Cost:  9.387262 \n",
      "Prediction:\n",
      " [[3.8326678]\n",
      " [3.6899655]\n",
      " [3.5672133]\n",
      " [3.4567587]\n",
      " [3.523206 ]\n",
      " [3.6051443]\n",
      " [3.3267105]\n",
      " [3.4977808]]\n",
      "41 Cost:  9.386605 \n",
      "Prediction:\n",
      " [[3.8325238]\n",
      " [3.6898224]\n",
      " [3.567093 ]\n",
      " [3.456664 ]\n",
      " [3.5230954]\n",
      " [3.605037 ]\n",
      " [3.326635 ]\n",
      " [3.4977055]]\n",
      "42 Cost:  9.385948 \n",
      "Prediction:\n",
      " [[3.8323798]\n",
      " [3.6896794]\n",
      " [3.5669725]\n",
      " [3.4565694]\n",
      " [3.5229847]\n",
      " [3.6049297]\n",
      " [3.3265593]\n",
      " [3.4976301]]\n",
      "43 Cost:  9.385291 \n",
      "Prediction:\n",
      " [[3.832236 ]\n",
      " [3.6895363]\n",
      " [3.566852 ]\n",
      " [3.4564748]\n",
      " [3.522874 ]\n",
      " [3.6048224]\n",
      " [3.3264837]\n",
      " [3.4975548]]\n",
      "44 Cost:  9.384634 \n",
      "Prediction:\n",
      " [[3.8320923]\n",
      " [3.689393 ]\n",
      " [3.566732 ]\n",
      " [3.4563804]\n",
      " [3.5227635]\n",
      " [3.6047153]\n",
      " [3.3264081]\n",
      " [3.4974794]]\n",
      "45 Cost:  9.383978 \n",
      "Prediction:\n",
      " [[3.8319483]\n",
      " [3.68925  ]\n",
      " [3.5666115]\n",
      " [3.4562855]\n",
      " [3.522653 ]\n",
      " [3.604608 ]\n",
      " [3.3263326]\n",
      " [3.497404 ]]\n",
      "46 Cost:  9.38332 \n",
      "Prediction:\n",
      " [[3.8318043]\n",
      " [3.689107 ]\n",
      " [3.5664911]\n",
      " [3.456191 ]\n",
      " [3.5225425]\n",
      " [3.6045008]\n",
      " [3.326257 ]\n",
      " [3.4973288]]\n",
      "47 Cost:  9.382664 \n",
      "Prediction:\n",
      " [[3.8316605]\n",
      " [3.688964 ]\n",
      " [3.566371 ]\n",
      " [3.4560964]\n",
      " [3.5224319]\n",
      " [3.6043935]\n",
      " [3.3261814]\n",
      " [3.4972532]]\n",
      "48 Cost:  9.382007 \n",
      "Prediction:\n",
      " [[3.8315167]\n",
      " [3.6888208]\n",
      " [3.5662508]\n",
      " [3.4560018]\n",
      " [3.5223212]\n",
      " [3.6042864]\n",
      " [3.3261058]\n",
      " [3.4971778]]\n",
      "49 Cost:  9.38135 \n",
      "Prediction:\n",
      " [[3.8313727]\n",
      " [3.6886778]\n",
      " [3.5661302]\n",
      " [3.455907 ]\n",
      " [3.5222106]\n",
      " [3.6041794]\n",
      " [3.3260303]\n",
      " [3.4971025]]\n",
      "50 Cost:  9.3806925 \n",
      "Prediction:\n",
      " [[3.8312287]\n",
      " [3.6885347]\n",
      " [3.56601  ]\n",
      " [3.4558125]\n",
      " [3.5221   ]\n",
      " [3.604072 ]\n",
      " [3.3259547]\n",
      " [3.4970272]]\n",
      "51 Cost:  9.380035 \n",
      "Prediction:\n",
      " [[3.831085 ]\n",
      " [3.6883917]\n",
      " [3.5658898]\n",
      " [3.4557178]\n",
      " [3.5219893]\n",
      " [3.6039648]\n",
      " [3.325879 ]\n",
      " [3.4969518]]\n",
      "52 Cost:  9.379379 \n",
      "Prediction:\n",
      " [[3.8309412]\n",
      " [3.6882486]\n",
      " [3.5657692]\n",
      " [3.4556231]\n",
      " [3.521879 ]\n",
      " [3.6038575]\n",
      " [3.3258035]\n",
      " [3.4968765]]\n",
      "53 Cost:  9.378723 \n",
      "Prediction:\n",
      " [[3.8307972]\n",
      " [3.6881056]\n",
      " [3.565649 ]\n",
      " [3.4555285]\n",
      " [3.521768 ]\n",
      " [3.6037505]\n",
      " [3.325728 ]\n",
      " [3.4968011]]\n",
      "54 Cost:  9.378065 \n",
      "Prediction:\n",
      " [[3.8306532]\n",
      " [3.6879625]\n",
      " [3.5655289]\n",
      " [3.4554338]\n",
      " [3.5216577]\n",
      " [3.6036432]\n",
      " [3.3256524]\n",
      " [3.4967258]]\n",
      "55 Cost:  9.377409 \n",
      "Prediction:\n",
      " [[3.8305094]\n",
      " [3.6878195]\n",
      " [3.5654085]\n",
      " [3.4553392]\n",
      " [3.521547 ]\n",
      " [3.603536 ]\n",
      " [3.3255768]\n",
      " [3.4966502]]\n",
      "56 Cost:  9.376753 \n",
      "Prediction:\n",
      " [[3.8303657]\n",
      " [3.6876764]\n",
      " [3.565288 ]\n",
      " [3.4552445]\n",
      " [3.5214365]\n",
      " [3.6034288]\n",
      " [3.3255012]\n",
      " [3.4965749]]\n",
      "57 Cost:  9.376097 \n",
      "Prediction:\n",
      " [[3.8302217]\n",
      " [3.6875334]\n",
      " [3.565168 ]\n",
      " [3.4551501]\n",
      " [3.5213258]\n",
      " [3.6033216]\n",
      " [3.3254256]\n",
      " [3.4964995]]\n",
      "58 Cost:  9.375441 \n",
      "Prediction:\n",
      " [[3.830078 ]\n",
      " [3.6873903]\n",
      " [3.5650477]\n",
      " [3.4550555]\n",
      " [3.5212154]\n",
      " [3.6032145]\n",
      " [3.32535  ]\n",
      " [3.4964242]]\n",
      "59 Cost:  9.374783 \n",
      "Prediction:\n",
      " [[3.8299341]\n",
      " [3.6872473]\n",
      " [3.5649273]\n",
      " [3.4549608]\n",
      " [3.5211048]\n",
      " [3.6031072]\n",
      " [3.3252745]\n",
      " [3.4963489]]\n",
      "60 Cost:  9.374126 \n",
      "Prediction:\n",
      " [[3.8297904]\n",
      " [3.6871042]\n",
      " [3.564807 ]\n",
      " [3.4548662]\n",
      " [3.5209942]\n",
      " [3.603    ]\n",
      " [3.325199 ]\n",
      " [3.4962735]]\n",
      "61 Cost:  9.37347 \n",
      "Prediction:\n",
      " [[3.8296463]\n",
      " [3.6869612]\n",
      " [3.5646868]\n",
      " [3.4547715]\n",
      " [3.5208836]\n",
      " [3.6028929]\n",
      " [3.3251233]\n",
      " [3.4961982]]\n",
      "62 Cost:  9.372814 \n",
      "Prediction:\n",
      " [[3.8295026]\n",
      " [3.6868181]\n",
      " [3.5645666]\n",
      " [3.4546769]\n",
      " [3.520773 ]\n",
      " [3.6027856]\n",
      " [3.3250477]\n",
      " [3.4961228]]\n",
      "63 Cost:  9.372158 \n",
      "Prediction:\n",
      " [[3.8293588]\n",
      " [3.686675 ]\n",
      " [3.5644462]\n",
      " [3.4545822]\n",
      " [3.5206625]\n",
      " [3.6026785]\n",
      " [3.3249722]\n",
      " [3.4960475]]\n",
      "64 Cost:  9.371501 \n",
      "Prediction:\n",
      " [[3.829215 ]\n",
      " [3.686532 ]\n",
      " [3.5643258]\n",
      " [3.4544878]\n",
      " [3.520552 ]\n",
      " [3.6025712]\n",
      " [3.3248966]\n",
      " [3.4959722]]\n",
      "65 Cost:  9.370846 \n",
      "Prediction:\n",
      " [[3.829071 ]\n",
      " [3.686389 ]\n",
      " [3.5642056]\n",
      " [3.4543931]\n",
      " [3.5204415]\n",
      " [3.6024642]\n",
      " [3.324821 ]\n",
      " [3.4958968]]\n",
      "66 Cost:  9.370188 \n",
      "Prediction:\n",
      " [[3.8289273]\n",
      " [3.6862462]\n",
      " [3.5640855]\n",
      " [3.4542985]\n",
      " [3.520331 ]\n",
      " [3.602357 ]\n",
      " [3.3247454]\n",
      " [3.4958212]]\n",
      "67 Cost:  9.369532 \n",
      "Prediction:\n",
      " [[3.8287835]\n",
      " [3.6861029]\n",
      " [3.563965 ]\n",
      " [3.4542038]\n",
      " [3.5202203]\n",
      " [3.6022499]\n",
      " [3.3246698]\n",
      " [3.495746 ]]\n",
      "68 Cost:  9.3688755 \n",
      "Prediction:\n",
      " [[3.8286397]\n",
      " [3.68596  ]\n",
      " [3.5638447]\n",
      " [3.4541092]\n",
      " [3.52011  ]\n",
      " [3.6021426]\n",
      " [3.3245943]\n",
      " [3.4956706]]\n",
      "69 Cost:  9.36822 \n",
      "Prediction:\n",
      " [[3.828496 ]\n",
      " [3.6858172]\n",
      " [3.5637245]\n",
      " [3.4540148]\n",
      " [3.5199993]\n",
      " [3.6020355]\n",
      " [3.3245187]\n",
      " [3.4955952]]\n",
      "70 Cost:  9.367565 \n",
      "Prediction:\n",
      " [[3.8283522]\n",
      " [3.6856742]\n",
      " [3.5636044]\n",
      " [3.4539201]\n",
      " [3.5198889]\n",
      " [3.6019282]\n",
      " [3.3244433]\n",
      " [3.4955199]]\n",
      "71 Cost:  9.366909 \n",
      "Prediction:\n",
      " [[3.8282084]\n",
      " [3.6855311]\n",
      " [3.5634842]\n",
      " [3.4538255]\n",
      " [3.5197783]\n",
      " [3.6018212]\n",
      " [3.3243678]\n",
      " [3.4954445]]\n",
      "72 Cost:  9.366253 \n",
      "Prediction:\n",
      " [[3.8280647]\n",
      " [3.685388 ]\n",
      " [3.5633638]\n",
      " [3.453731 ]\n",
      " [3.5196676]\n",
      " [3.6017141]\n",
      " [3.3242922]\n",
      " [3.4953692]]\n",
      "73 Cost:  9.365597 \n",
      "Prediction:\n",
      " [[3.827921 ]\n",
      " [3.685245 ]\n",
      " [3.5632436]\n",
      " [3.4536364]\n",
      " [3.519557 ]\n",
      " [3.6016068]\n",
      " [3.3242166]\n",
      " [3.4952939]]\n",
      "74 Cost:  9.364941 \n",
      "Prediction:\n",
      " [[3.8277771]\n",
      " [3.6851022]\n",
      " [3.5631235]\n",
      " [3.4535418]\n",
      " [3.5194466]\n",
      " [3.6014996]\n",
      " [3.324141 ]\n",
      " [3.4952185]]\n",
      "75 Cost:  9.364285 \n",
      "Prediction:\n",
      " [[3.8276334]\n",
      " [3.6849592]\n",
      " [3.563003 ]\n",
      " [3.453447 ]\n",
      " [3.5193362]\n",
      " [3.6013925]\n",
      " [3.3240654]\n",
      " [3.4951432]]\n",
      "76 Cost:  9.363629 \n",
      "Prediction:\n",
      " [[3.8274896]\n",
      " [3.6848164]\n",
      " [3.562883 ]\n",
      " [3.4533525]\n",
      " [3.5192256]\n",
      " [3.6012855]\n",
      " [3.3239899]\n",
      " [3.4950678]]\n",
      "77 Cost:  9.362974 \n",
      "Prediction:\n",
      " [[3.8273458]\n",
      " [3.6846733]\n",
      " [3.5627627]\n",
      " [3.453258 ]\n",
      " [3.519115 ]\n",
      " [3.6011782]\n",
      " [3.3239143]\n",
      " [3.4949925]]\n",
      "78 Cost:  9.362318 \n",
      "Prediction:\n",
      " [[3.827202 ]\n",
      " [3.6845303]\n",
      " [3.5626423]\n",
      " [3.4531634]\n",
      " [3.5190046]\n",
      " [3.6010711]\n",
      " [3.3238387]\n",
      " [3.4949172]]\n",
      "79 Cost:  9.361662 \n",
      "Prediction:\n",
      " [[3.8270583]\n",
      " [3.6843874]\n",
      " [3.5625222]\n",
      " [3.4530687]\n",
      " [3.518894 ]\n",
      " [3.600964 ]\n",
      " [3.3237631]\n",
      " [3.4948416]]\n",
      "80 Cost:  9.361006 \n",
      "Prediction:\n",
      " [[3.8269145]\n",
      " [3.6842444]\n",
      " [3.562402 ]\n",
      " [3.4529743]\n",
      " [3.5187836]\n",
      " [3.6008568]\n",
      " [3.3236876]\n",
      " [3.4947662]]\n",
      "81 Cost:  9.360351 \n",
      "Prediction:\n",
      " [[3.8267708]\n",
      " [3.6841013]\n",
      " [3.5622816]\n",
      " [3.4528797]\n",
      " [3.518673 ]\n",
      " [3.6007497]\n",
      " [3.3236122]\n",
      " [3.494691 ]]\n",
      "82 Cost:  9.359695 \n",
      "Prediction:\n",
      " [[3.826627 ]\n",
      " [3.6839585]\n",
      " [3.5621614]\n",
      " [3.452785 ]\n",
      " [3.5185623]\n",
      " [3.6006427]\n",
      " [3.3235364]\n",
      " [3.4946156]]\n",
      "83 Cost:  9.359039 \n",
      "Prediction:\n",
      " [[3.8264832]\n",
      " [3.6838155]\n",
      " [3.5620413]\n",
      " [3.4526904]\n",
      " [3.518452 ]\n",
      " [3.6005354]\n",
      " [3.323461 ]\n",
      " [3.4945402]]\n",
      "84 Cost:  9.358385 \n",
      "Prediction:\n",
      " [[3.8263395]\n",
      " [3.6836727]\n",
      " [3.5619211]\n",
      " [3.452596 ]\n",
      " [3.5183415]\n",
      " [3.6004283]\n",
      " [3.3233855]\n",
      " [3.4944649]]\n",
      "85 Cost:  9.357729 \n",
      "Prediction:\n",
      " [[3.8261957]\n",
      " [3.6835296]\n",
      " [3.561801 ]\n",
      " [3.4525013]\n",
      " [3.518231 ]\n",
      " [3.600321 ]\n",
      " [3.32331  ]\n",
      " [3.4943895]]\n",
      "86 Cost:  9.357073 \n",
      "Prediction:\n",
      " [[3.826052 ]\n",
      " [3.6833866]\n",
      " [3.5616806]\n",
      " [3.4524066]\n",
      " [3.5181203]\n",
      " [3.600214 ]\n",
      " [3.3232343]\n",
      " [3.4943142]]\n",
      "87 Cost:  9.356418 \n",
      "Prediction:\n",
      " [[3.8259082]\n",
      " [3.6832438]\n",
      " [3.5615604]\n",
      " [3.4523122]\n",
      " [3.51801  ]\n",
      " [3.6001067]\n",
      " [3.3231587]\n",
      " [3.4942389]]\n",
      "88 Cost:  9.3557625 \n",
      "Prediction:\n",
      " [[3.8257644]\n",
      " [3.6831007]\n",
      " [3.56144  ]\n",
      " [3.4522176]\n",
      " [3.5178993]\n",
      " [3.5999997]\n",
      " [3.3230832]\n",
      " [3.4941635]]\n",
      "89 Cost:  9.355106 \n",
      "Prediction:\n",
      " [[3.8256207]\n",
      " [3.6829576]\n",
      " [3.5613198]\n",
      " [3.452123 ]\n",
      " [3.517789 ]\n",
      " [3.5998926]\n",
      " [3.3230076]\n",
      " [3.4940882]]\n",
      "90 Cost:  9.35445 \n",
      "Prediction:\n",
      " [[3.825477 ]\n",
      " [3.6828148]\n",
      " [3.5611997]\n",
      " [3.4520283]\n",
      " [3.5176783]\n",
      " [3.5997853]\n",
      " [3.322932 ]\n",
      " [3.4940128]]\n",
      "91 Cost:  9.353796 \n",
      "Prediction:\n",
      " [[3.825333 ]\n",
      " [3.682672 ]\n",
      " [3.5610795]\n",
      " [3.4519339]\n",
      " [3.5175679]\n",
      " [3.5996783]\n",
      " [3.3228564]\n",
      " [3.4939375]]\n",
      "92 Cost:  9.353142 \n",
      "Prediction:\n",
      " [[3.8251896]\n",
      " [3.682529 ]\n",
      " [3.5609593]\n",
      " [3.4518392]\n",
      " [3.5174575]\n",
      " [3.5995712]\n",
      " [3.322781 ]\n",
      " [3.4938622]]\n",
      "93 Cost:  9.352486 \n",
      "Prediction:\n",
      " [[3.8250458]\n",
      " [3.6823862]\n",
      " [3.5608392]\n",
      " [3.4517448]\n",
      " [3.5173469]\n",
      " [3.5994642]\n",
      " [3.3227053]\n",
      " [3.4937868]]\n",
      "94 Cost:  9.351831 \n",
      "Prediction:\n",
      " [[3.824902 ]\n",
      " [3.6822433]\n",
      " [3.560719 ]\n",
      " [3.4516501]\n",
      " [3.5172362]\n",
      " [3.5993571]\n",
      " [3.32263  ]\n",
      " [3.4937115]]\n",
      "95 Cost:  9.351175 \n",
      "Prediction:\n",
      " [[3.8247583]\n",
      " [3.6821003]\n",
      " [3.5605989]\n",
      " [3.4515555]\n",
      " [3.5171258]\n",
      " [3.5992498]\n",
      " [3.3225543]\n",
      " [3.4936361]]\n",
      "96 Cost:  9.350521 \n",
      "Prediction:\n",
      " [[3.8246148]\n",
      " [3.6819572]\n",
      " [3.5604787]\n",
      " [3.451461 ]\n",
      " [3.5170155]\n",
      " [3.5991428]\n",
      " [3.3224788]\n",
      " [3.4935608]]\n",
      "97 Cost:  9.349865 \n",
      "Prediction:\n",
      " [[3.824471 ]\n",
      " [3.6818144]\n",
      " [3.5603585]\n",
      " [3.4513664]\n",
      " [3.5169048]\n",
      " [3.5990357]\n",
      " [3.3224032]\n",
      " [3.4934855]]\n",
      "98 Cost:  9.34921 \n",
      "Prediction:\n",
      " [[3.8243272]\n",
      " [3.6816716]\n",
      " [3.5602381]\n",
      " [3.4512718]\n",
      " [3.5167942]\n",
      " [3.5989285]\n",
      " [3.3223276]\n",
      " [3.49341  ]]\n",
      "99 Cost:  9.348556 \n",
      "Prediction:\n",
      " [[3.8241835]\n",
      " [3.6815286]\n",
      " [3.560118 ]\n",
      " [3.4511774]\n",
      " [3.5166838]\n",
      " [3.5988214]\n",
      " [3.322252 ]\n",
      " [3.4933348]]\n",
      "100 Cost:  9.3479 \n",
      "Prediction:\n",
      " [[3.82404  ]\n",
      " [3.6813858]\n",
      " [3.5599978]\n",
      " [3.4510827]\n",
      " [3.5165734]\n",
      " [3.5987144]\n",
      " [3.3221765]\n",
      " [3.4932594]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder('float', [None, 4])\n",
    "Y = tf.placeholder('float', [None, 1])\n",
    "W = tf.Variable(tf.random_normal([4,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(101):\n",
    "        cost_val, hy_val, _ = sess.run([cost, hypothesis, optimizer], \n",
    "                                       feed_dict = {X: x_data, Y: y_data})\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  0001 cost =  3.127368403\n",
      "Accuracy:  0.7513\n",
      "Epoch:  0002 cost =  1.162718924\n",
      "Accuracy:  0.8119\n",
      "Epoch:  0003 cost =  0.905096177\n",
      "Accuracy:  0.8352\n",
      "Epoch:  0004 cost =  0.783722736\n",
      "Accuracy:  0.8498\n",
      "Epoch:  0005 cost =  0.710221513\n",
      "Accuracy:  0.8587\n",
      "Epoch:  0006 cost =  0.659093270\n",
      "Accuracy:  0.8649\n",
      "Epoch:  0007 cost =  0.620576099\n",
      "Accuracy:  0.8692\n",
      "Epoch:  0008 cost =  0.589756200\n",
      "Accuracy:  0.8728\n",
      "Epoch:  0009 cost =  0.564480219\n",
      "Accuracy:  0.8755\n",
      "Epoch:  0010 cost =  0.543443129\n",
      "Accuracy:  0.8776\n",
      "Epoch:  0011 cost =  0.525296722\n",
      "Accuracy:  0.8795\n",
      "Epoch:  0012 cost =  0.509535902\n",
      "Accuracy:  0.8809\n",
      "Epoch:  0013 cost =  0.495670841\n",
      "Accuracy:  0.8822\n",
      "Epoch:  0014 cost =  0.483791041\n",
      "Accuracy:  0.8839\n",
      "Epoch:  0015 cost =  0.472195948\n",
      "Accuracy:  0.884\n",
      "learning finished!\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "mnist data\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "#one_hot = True이면 별도로 one hot으로 처리하지 않아도 알아서 one hot으로 처리된다.\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])#28*28 = 784\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "#cross entropy\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "#정확한지 아닌지를 평가, one hot 값인 hypothesis와 y값을 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "#정확도\n",
    "\n",
    "training_epochs = 15 \n",
    "#전체 데이터셋을 한 번 다 학습시킨 것(한 번 다 돈 것)이 1 epoch\n",
    "#학습 example이 1000개 있다, batch_size = 500이면 epoch = 2를 해야함\n",
    "batch_size = 100\n",
    "#데이터가 너무 많아서 한꺼번에 불러올 수는 없음\n",
    "#조금씩 batch로 잘라서 학습시킴\n",
    "#한 번에 몇 개씩을 잘라서 학습시킬까임\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch): #iteration\n",
    "            #iteration을 몇 번 돌까? -> 전체 데이터수/batch_size\n",
    "            '''\n",
    "            전체 사이즈: 1000개, batch_size = 100이면\n",
    "            몇 번 루프를 iteration하면 1 epoch이 되는가?\n",
    "            100번을 돌면 1epoch을 돈다\n",
    "            '''\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            #100개씩 읽어와서 학습을 시켜준다.\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict = {X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        print('Epoch: ', '%04d' %(epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "        print('Accuracy: ', accuracy.eval(session = sess,\n",
    "                feed_dict = {X: mnist.test.images, Y: mnist.test.labels}))\n",
    "        \n",
    "    print('learning finished!')\n",
    "\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print('Label: ', sess.run(tf.argmax(mnist.test.labels[r: r+1], 1)))\n",
    "    print('Prediction: ', sess.run(tf.argmax(hypothesis, 1),\n",
    "                                  feed_dict = {X: mnist.test.images[r: r+1]}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r: r+1].\n",
    "              reshape(28, 28), cmap = 'Greys', interpolation = 'nearest')\n",
    "\n",
    "    plt.show()\n",
    "#c = random.randint(0, mnist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
